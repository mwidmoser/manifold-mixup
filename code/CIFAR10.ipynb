{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CIFAR10.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"bxCe-p9LIXRu","colab_type":"text"},"source":["# Assignment 3 (15 points)\n","(due on Nov. 25, 11pm)\n","\n","In the third assignment, we will write a Convolutional Neural Network (CNN) together with training and evaluation routines.\n","\n","The task description can be found below.\n","\n","**Important**: I strongly recommend to use *Google Collab (GC)* for this assignment. Make yourself familiar with running Jupyter notebooks on GC (especially selecting the right runtime, i.e., Python 3 + GPU). This will make your life a lot easier, as training will be faster and you can easily debug problems in your model."]},{"cell_type":"code","metadata":{"id":"CeswLxlcE28m","colab_type":"code","outputId":"79bcb480-faee-4f0f-a7f4-aebf7ad964df","executionInfo":{"status":"ok","timestamp":1574847603480,"user_tz":-60,"elapsed":3492,"user":{"displayName":"Roland Kwitt","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBf7BGV1l2b7ymP7ol7FhK8GOHj3X5u0PyNhETx=s64","userId":"17953440578497129710"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","# PyTorch imports\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","# Torchvision imports\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision.datasets import MNIST, CIFAR10\n","from torch.utils.data.dataset import Subset\n","from torch.utils.data import DataLoader\n","\n","# Numpy and other stuff\n","import numpy as np\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from collections import Counter\n","\n","torch.manual_seed(1234);\n","np.random.seed(1234);\n","\n","# Check if we have a CUDA-capable device; if so, use it\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('Will train on {}'.format(device))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Will train on cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RvUP3dkLL76A","colab_type":"code","outputId":"93c317be-68cf-44fe-8324-cffbabc1c108","executionInfo":{"status":"ok","timestamp":1574847606515,"user_tz":-60,"elapsed":922,"user":{"displayName":"Roland Kwitt","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBf7BGV1l2b7ymP7ol7FhK8GOHj3X5u0PyNhETx=s64","userId":"17953440578497129710"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(torch.__version__)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["1.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eOSdQelkMPAd","colab_type":"code","outputId":"25a4acdb-1b82-4a6a-873c-eac048ee1d13","executionInfo":{"status":"ok","timestamp":1574847680860,"user_tz":-60,"elapsed":27545,"user":{"displayName":"Roland Kwitt","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBf7BGV1l2b7ymP7ol7FhK8GOHj3X5u0PyNhETx=s64","userId":"17953440578497129710"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["# CIFAR10 transforms (random horizontal flipping + mean/std. dev. normalize)\n","cifar10_transforms = transforms.Compose(\n","    [transforms.RandomHorizontalFlip(p=0.5),\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n","         \n","# Load full training data\n","ds_train = CIFAR10('./cifar', \n","                 train=True, \n","                 transform=cifar10_transforms, \n","                 target_transform=None, \n","                 download=True)\n","\n","# Load full testing data \n","ds_test = CIFAR10('./cifar', \n","                 train=False, \n","                 transform=cifar10_transforms,\n","                 target_transform=None, \n","                 download=True)\n","\n","lab = [ds_train[x][1] for x in range(len(ds_train))]"],"execution_count":3,"outputs":[{"output_type":"stream","text":["\r0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":[" 99%|█████████▊| 168026112/170498071 [00:11<00:00, 17343285.75it/s]"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./cifar/cifar-10-python.tar.gz to ./cifar\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CGmz9GEKRuvw","colab_type":"code","colab":{}},"source":["def generate_train_indices(n_splits, train_size, lab):\n","    s = StratifiedShuffleSplit(\n","        n_splits=n_splits, \n","        train_size=train_size, \n","        test_size=None)\n","    \n","    return [i.tolist() for i, _ in s.split(lab, lab)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KGYPAsbwR77Z","colab_type":"code","colab":{}},"source":["classes = ['plane', \n","           'car', \n","           'bird', \n","           'cat',\n","           'deer', \n","           'dog', \n","           'frog', \n","           'horse', \n","           'ship', \n","           'truck']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vxTbKsZnSC-2","colab_type":"code","colab":{}},"source":["def show_images(ds: torchvision.datasets.cifar.CIFAR10, \n","                indices: list):\n","    \n","    assert np.max(indices) < len(ds)\n","    \n","    plt.figure(figsize=(9, len(indices)));\n","    for j,idx in enumerate(indices):\n","        plt.subplot(1,len(indices),j+1)\n","        plt.imshow(ds[idx][0].permute(1,2,0).numpy())\n","        plt.title('Label={}'.format(classes[ds[idx][1]]),fontsize=9)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4ElEkUGgJ86C","colab_type":"text"},"source":["## Tasks\n","\n","The assignment is split into 4 parts: \n","\n","1. Writing the model definition\n","2. Writing the training code\n","3. Writing the testing code\n","4. Writing the *glue* code for training/testing\n","\n","*First*, implement the following **convolutional neural network (CNN)**: It consists of 3 blocks and a simple linear classifier at the end.\n","\n","My notation denotes the following:\n","\n","- `Conv2D(in_channels, out_channels, kernel_size, padding)` - 2D Convolution\n","- `MaxPool(kernel_size, stride, padding)` - Max. pooling\n","- `AvgPool(kernel_size, stride, padding)` - Avg. pooling\n","- `Dropout(dropout_probability)` - Dropout layer\n","- `BatchNorm2D` - 2D batch normalization\n","\n","All these operations can also be found in the [PyTorch documentaton](https://pytorch.org/docs/stable/index.html).\n","\n","**Block 1**\n","\n","```\n","Conv2D(  3,128,3,1) -> Batchnorm2D -> LeakyReLU(0.1)\n","Conv2D(128,128,3,1) -> Batchnorm2D -> LeakyReLU(0.1)\n","Conv2D(128,128,3,1) -> Batchnorm2D -> LeakyReLU(0.1)\n","MaxPool(2,2,0)\n","Dropout(0.5)\n","```\n","The output size at that point should be $N \\times 128 \\times 16 \\times 16$.\n","\n","**Block 2**\n","\n","```\n","Conv2D(128,256,3,1) -> Batchnorm2D -> LeakyReLU(0.1)\n","Conv2D(256,256,3,1) -> Batchnorm2D -> LeakyReLU(0.1)\n","Conv2D(256,256,3,1) -> Batchnorm2D -> LeakyReLU(0.1)\n","MaxPool(2,2,0)\n","Dropout(0.5)\n","```\n","The output size at that point should be $N \\times 128 \\times 8 \\times 8$.\n","\n","**Block 3**\n","\n","```\n","Conv2D(256,512,3,0) -> Batchnorm2D -> LeakyReLU(0.1)\n","Conv2D(512,256,1,0) -> Batchnorm2D -> LeakyReLU(0.1)\n","Conv2D(256,128,1,0) -> Batchnorm2D -> LeakyReLU(0.1)\n","AvgPool(6,2,0)\n","Dropout(0.5)\n","```\n","The output size at that point should be $N \\times 128 \\times 1 \\times 1$.\n","\n","**Classifier**\n","\n","View the output of the last block as a $1 \\times 128$ tensor and add a \n","linear layer mapping from $\\mathbb{R}^{128} \\rightarrow \\mathbb{R}^{10}$\n","(include bias)."]},{"cell_type":"markdown","metadata":{"id":"YDCXdZUSKzAY","colab_type":"text"},"source":["```python\n","class ConvNet(nn.Module): \n","    def __init__(self, num_classes=10):\n","      super(ConvNet, self).__init__()\n","      # YOUR CODE GOES HERE\n","      \n","    def forward(self, x):\n","      # YOUR CODE GOES HERE\n","      pass\n","```"]},{"cell_type":"code","metadata":{"id":"nmEQ00e8SEv2","colab_type":"code","colab":{}},"source":["class ConvNet(nn.Module): \n","    def __init__(self, num_classes=10):\n","        super(ConvNet, self).__init__()\n","        \n","        def make_block(conv_config, pooling_op=None, use_dropout=False):\n","            mlist = nn.ModuleList()\n","            for in_c, out_c, k_size, pad in conv_config:\n","                mlist.extend([\n","                    nn.Conv2d(in_c, out_c, k_size, padding=pad),\n","                    nn.BatchNorm2d(out_c),\n","                    nn.LeakyReLU(0.1)\n","                ])\n","            mlist.append(pooling_op)\n","            if use_dropout:\n","                mlist.append(nn.Dropout(0.5))\n","            return mlist\n","\n","        self.block1 = make_block([\n","            [  3,128,3,1],\n","            [128,128,3,1],\n","            [128,128,3,1]], \n","            nn.MaxPool2d(2,stride=2,padding=0), \n","            use_dropout=True)\n","        \n","        self.block2 = make_block([\n","            [128,256,3,1],\n","            [256,256,3,1],\n","            [256,256,3,1]], \n","            nn.MaxPool2d(2,stride=2,padding=0),\n","            use_dropout=True)\n","\n","        self.block3 = make_block([\n","            [256,512,3,0],\n","            [512,256,1,0],\n","            [256,128,1,0]], \n","            nn.AvgPool2d(6,stride=2,padding=0),\n","            use_dropout=False)\n","        \n","        self.classifier = nn.Linear(128,10)\n","    \n","    def forward(self, x):\n","        for l in self.block1: x = l(x)\n","        for l in self.block2: x = l(x)\n","        for l in self.block3: x = l(x)\n","        x = x.view(x.size(0),-1)\n","        x = self.classifier(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BhtMSe7TSHo6","colab_type":"code","outputId":"0ba52a0f-f27c-4d2c-fc5b-aac1bcdea340","executionInfo":{"status":"ok","timestamp":1574847831009,"user_tz":-60,"elapsed":796,"user":{"displayName":"Roland Kwitt","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBf7BGV1l2b7ymP7ol7FhK8GOHj3X5u0PyNhETx=s64","userId":"17953440578497129710"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["net = ConvNet(10)\n","out = net(torch.rand(5,3,32,32))\n","print(out.size())"],"execution_count":8,"outputs":[{"output_type":"stream","text":["torch.Size([5, 10])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"duG26vY27GJ9","colab_type":"text"},"source":["Write a **training method**\n","\n","```python\n","def train(model, device, train_loader, optimizer, epoch):\n","  # your code goes here\n","```\n","\n","which takes the `model`, the `device`, the current loader for the training data, the `optimizer` and the current epoch as parameters.\n","\n","The training method should also print the accumulated cross-entropy loss over each epoch.\n","\n","Then, write a **testing method**\n","\n","```python\n","def test(model, device, test_loader):\n","  # your code goes here\n","```\n","\n","which takes the `model`, the `device` and the testing data loader as parameters and evaluates the model on the testing split of CIFAR10.\n","\n","*For both methods, you can use my MNIST Jupyter notebook \n","as a template.*\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"vB6ibYt1SMhg","colab_type":"code","colab":{}},"source":["def train(model, device, train_loader, optimizer, epoch):\n","    \n","    model.train()\n","    \n","    epoch_loss = 0\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        \n","        data, target = data.to(device), target.to(device)\n","        \n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.cross_entropy(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","\n","    print('Train Epoch: {:2d} \\tLoss: {:.6f}'.format(epoch, epoch_loss))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HsHPvlm1SVht","colab_type":"code","colab":{}},"source":["def test(model, device, test_loader):\n","    model.eval()\n","    test_loss, correct = 0, 0\n","    \n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n","            pred = output.argmax(dim=1, keepdim=True)\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hByDU-eF7rjY","colab_type":"text"},"source":["Finally, write the *glue* code which iterates over `n_epochs` (e.g., 100) and, in each epoch, calls `train(...)` and `test(...)`."]},{"cell_type":"code","metadata":{"id":"OXEkyYPjSPye","colab_type":"code","outputId":"017fa055-6bed-4d22-c05f-8ac3d4c87eb7","executionInfo":{"status":"ok","timestamp":1574848260755,"user_tz":-60,"elapsed":914,"user":{"displayName":"Roland Kwitt","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBf7BGV1l2b7ymP7ol7FhK8GOHj3X5u0PyNhETx=s64","userId":"17953440578497129710"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["train_indices = generate_train_indices(10, 500, lab)\n","ds_train_subset = Subset(ds_train, train_indices[1])\n","print(Counter([ds_train_subset[i][1] for i in range(len(ds_train_subset))]))\n","\n","train_loader = torch.utils.data.DataLoader(\n","    ds_train_subset,\n","    batch_size=32,\n","    shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(\n","    ds_test, \n","    batch_size=64, \n","    shuffle=False)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Counter({2: 50, 3: 50, 8: 50, 6: 50, 9: 50, 0: 50, 4: 50, 7: 50, 1: 50, 5: 50})\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"twFNwCY46vO5","colab_type":"text"},"source":["Train the model using **SGD** with a learning rate of 0.01 and momentum of 0.9 for 100 epochs.\n","\n","After every 10th epoch, evaluate the current model on the testing data and print the current accuracy."]},{"cell_type":"code","metadata":{"id":"Cx2ahoZnSRqP","colab_type":"code","outputId":"0ba12fa9-dc66-44ca-8147-202b482ad2b6","executionInfo":{"status":"ok","timestamp":1573722386555,"user_tz":-60,"elapsed":174874,"user":{"displayName":"Roland Kwitt","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBf7BGV1l2b7ymP7ol7FhK8GOHj3X5u0PyNhETx=s64","userId":"17953440578497129710"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["n_epochs = 100\n","\n","model = ConvNet().to(device)\n","\n","optimizer = optim.SGD(\n","    model.parameters(), \n","    lr=0.01, \n","    momentum=0.9,\n","    weight_decay=1e-3)\n","\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 30, 0.1)\n","\n","for epoch in range(1,n_epochs + 1):\n","    train(model, device, train_loader, optimizer, epoch)\n","    if epoch % 10 == 0:\n","      test(model, device, test_loader)\n","    scheduler.step()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train Epoch:  1 \tLoss: 35.070380\n","Train Epoch:  2 \tLoss: 31.259755\n","Train Epoch:  3 \tLoss: 29.924989\n","Train Epoch:  4 \tLoss: 28.908141\n","Train Epoch:  5 \tLoss: 27.579933\n","Train Epoch:  6 \tLoss: 25.884531\n","Train Epoch:  7 \tLoss: 25.010489\n","Train Epoch:  8 \tLoss: 24.257061\n","Train Epoch:  9 \tLoss: 22.806054\n","Train Epoch: 10 \tLoss: 22.769443\n","\n","Test set: Average loss: 1.8360, Accuracy: 3483/10000 (35%)\n","\n","Train Epoch: 11 \tLoss: 22.033722\n","Train Epoch: 12 \tLoss: 20.440476\n","Train Epoch: 13 \tLoss: 20.216068\n","Train Epoch: 14 \tLoss: 19.459319\n","Train Epoch: 15 \tLoss: 18.544853\n","Train Epoch: 16 \tLoss: 18.764824\n","Train Epoch: 17 \tLoss: 19.586009\n","Train Epoch: 18 \tLoss: 19.042756\n","Train Epoch: 19 \tLoss: 17.903930\n","Train Epoch: 20 \tLoss: 16.870466\n","\n","Test set: Average loss: 1.8044, Accuracy: 3938/10000 (39%)\n","\n","Train Epoch: 21 \tLoss: 16.183145\n","Train Epoch: 22 \tLoss: 16.309255\n","Train Epoch: 23 \tLoss: 15.619540\n","Train Epoch: 24 \tLoss: 13.994100\n","Train Epoch: 25 \tLoss: 13.106648\n","Train Epoch: 26 \tLoss: 12.964822\n","Train Epoch: 27 \tLoss: 12.635393\n","Train Epoch: 28 \tLoss: 13.302823\n","Train Epoch: 29 \tLoss: 13.007268\n","Train Epoch: 30 \tLoss: 12.106289\n","\n","Test set: Average loss: 1.7936, Accuracy: 4371/10000 (44%)\n","\n","Train Epoch: 31 \tLoss: 9.339387\n","Train Epoch: 32 \tLoss: 8.472800\n","Train Epoch: 33 \tLoss: 7.916734\n","Train Epoch: 34 \tLoss: 7.161044\n","Train Epoch: 35 \tLoss: 7.876714\n","Train Epoch: 36 \tLoss: 7.515478\n","Train Epoch: 37 \tLoss: 6.935014\n","Train Epoch: 38 \tLoss: 8.002928\n","Train Epoch: 39 \tLoss: 7.171401\n","Train Epoch: 40 \tLoss: 6.402471\n","\n","Test set: Average loss: 1.6744, Accuracy: 4732/10000 (47%)\n","\n","Train Epoch: 41 \tLoss: 6.712245\n","Train Epoch: 42 \tLoss: 6.593199\n","Train Epoch: 43 \tLoss: 6.474732\n","Train Epoch: 44 \tLoss: 6.280345\n","Train Epoch: 45 \tLoss: 6.282797\n","Train Epoch: 46 \tLoss: 6.403470\n","Train Epoch: 47 \tLoss: 6.487419\n","Train Epoch: 48 \tLoss: 6.630654\n","Train Epoch: 49 \tLoss: 6.207112\n","Train Epoch: 50 \tLoss: 5.493481\n","\n","Test set: Average loss: 1.7259, Accuracy: 4664/10000 (47%)\n","\n","Train Epoch: 51 \tLoss: 5.822213\n","Train Epoch: 52 \tLoss: 5.575886\n","Train Epoch: 53 \tLoss: 6.388335\n","Train Epoch: 54 \tLoss: 5.625674\n","Train Epoch: 55 \tLoss: 5.572734\n","Train Epoch: 56 \tLoss: 5.596490\n","Train Epoch: 57 \tLoss: 5.185795\n","Train Epoch: 58 \tLoss: 4.890915\n","Train Epoch: 59 \tLoss: 4.980781\n","Train Epoch: 60 \tLoss: 4.348174\n","\n","Test set: Average loss: 1.7330, Accuracy: 4755/10000 (48%)\n","\n","Train Epoch: 61 \tLoss: 5.249169\n","Train Epoch: 62 \tLoss: 4.979495\n","Train Epoch: 63 \tLoss: 4.560950\n","Train Epoch: 64 \tLoss: 4.772828\n","Train Epoch: 65 \tLoss: 4.931568\n","Train Epoch: 66 \tLoss: 4.684692\n","Train Epoch: 67 \tLoss: 4.539293\n","Train Epoch: 68 \tLoss: 4.410664\n","Train Epoch: 69 \tLoss: 4.475440\n","Train Epoch: 70 \tLoss: 4.525977\n","\n","Test set: Average loss: 1.7465, Accuracy: 4762/10000 (48%)\n","\n","Train Epoch: 71 \tLoss: 4.723138\n","Train Epoch: 72 \tLoss: 5.019125\n","Train Epoch: 73 \tLoss: 4.772735\n","Train Epoch: 74 \tLoss: 4.665008\n","Train Epoch: 75 \tLoss: 4.507285\n","Train Epoch: 76 \tLoss: 4.523920\n","Train Epoch: 77 \tLoss: 4.652721\n","Train Epoch: 78 \tLoss: 4.449150\n","Train Epoch: 79 \tLoss: 4.757888\n","Train Epoch: 80 \tLoss: 4.297962\n","\n","Test set: Average loss: 1.7536, Accuracy: 4747/10000 (47%)\n","\n","Train Epoch: 81 \tLoss: 4.659675\n","Train Epoch: 82 \tLoss: 4.082174\n","Train Epoch: 83 \tLoss: 4.331036\n","Train Epoch: 84 \tLoss: 4.147208\n","Train Epoch: 85 \tLoss: 4.428154\n","Train Epoch: 86 \tLoss: 4.487133\n","Train Epoch: 87 \tLoss: 4.673340\n","Train Epoch: 88 \tLoss: 4.450648\n","Train Epoch: 89 \tLoss: 4.554719\n","Train Epoch: 90 \tLoss: 4.250499\n","\n","Test set: Average loss: 1.7464, Accuracy: 4802/10000 (48%)\n","\n","Train Epoch: 91 \tLoss: 4.430908\n","Train Epoch: 92 \tLoss: 4.523680\n","Train Epoch: 93 \tLoss: 4.616660\n","Train Epoch: 94 \tLoss: 4.214405\n","Train Epoch: 95 \tLoss: 4.752489\n","Train Epoch: 96 \tLoss: 4.514444\n","Train Epoch: 97 \tLoss: 4.065506\n","Train Epoch: 98 \tLoss: 4.573420\n","Train Epoch: 99 \tLoss: 4.542274\n","Train Epoch: 100 \tLoss: 4.645363\n","\n","Test set: Average loss: 1.7545, Accuracy: 4782/10000 (48%)\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bAWY0tei5QGW","colab_type":"code","outputId":"b8d0725a-b9de-4e31-98bb-128ebeef2aea","executionInfo":{"status":"ok","timestamp":1573567998535,"user_tz":-60,"elapsed":5378,"user":{"displayName":"Roland Kwitt","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBf7BGV1l2b7ymP7ol7FhK8GOHj3X5u0PyNhETx=s64","userId":"17953440578497129710"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["test(model, device, test_loader)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Test set: Average loss: 1.8020, Accuracy: 4961/10000 (50%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Pft0JDgJ9mgz","colab_type":"text"},"source":["If you train with reasonable settings, you should get a testing accuracy somewhere between 45% and 50% (random chance is 1/10 obviously)."]}]}